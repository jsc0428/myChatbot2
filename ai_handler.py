import os
from openai import OpenAI
import streamlit as st
from typing import List, Dict, Any, Optional

def get_model_templates() -> Dict[str, Dict[str, Any]]:
    """OpenAI ëª¨ë¸ í…œí”Œë¦¿ ì •ì˜ - 2025ë…„ ìµœì‹  ëª¨ë¸ ë°˜ì˜"""
    return {
        "ğŸ§  Reasoning Models": {
            "o3": {
                "name": "o3",
                "description": "ê°€ì¥ ê°•ë ¥í•œ ì¶”ë¡  ëª¨ë¸ (ë³µì¡í•œ ìˆ˜í•™, ê³¼í•™, ì½”ë”© ë¬¸ì œì— ìµœì í™”)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o4-mini": {
                "name": "o4-mini",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì¶”ë¡  ëª¨ë¸ (ë©€í‹°ëª¨ë‹¬ ì§€ì›, ë„êµ¬ í†µí•©)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o3-mini": {
                "name": "o3-mini",
                "description": "o3ì˜ ì†Œí˜• ëŒ€ì•ˆ ëª¨ë¸ (ì¶”ë¡  ëŠ¥ë ¥ ìœ ì§€, ë¹„ìš© íš¨ìœ¨ì )",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o1": {
                "name": "o1",
                "description": "ì´ì „ o-ì‹œë¦¬ì¦ˆ ì¶”ë¡  ëª¨ë¸ (ì•ˆì •ì ì¸ ì¶”ë¡  ì„±ëŠ¥)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o1-mini": {
                "name": "o1-mini",
                "description": "o1ì˜ ì†Œí˜• ëŒ€ì•ˆ (ì½”ë”© ë° ìˆ˜í•™ ë¬¸ì œì— íŠ¹í™”) - Deprecated",
                "max_tokens": 65536,
                "supports_streaming": False,
                "context_window": 128000,
                "deprecated": True
            }
        },
        "ğŸš€ Flagship Chat Models": {
            "gpt-4.1": {
                "name": "gpt-4.1",
                "description": "ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ í”Œë˜ê·¸ì‹­ GPT ëª¨ë¸ (ìµœê³  ì„±ëŠ¥)",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576
            },
            "gpt-4o": {
                "name": "gpt-4o",
                "description": "ë¹ ë¥´ê³  ì§€ëŠ¥ì ì´ë©° ìœ ì—°í•œ GPT ëª¨ë¸ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000
            },
            "gpt-4o-audio": {
                "name": "gpt-4o-audio",
                "description": "GPT-4o ì˜¤ë””ì˜¤ ì…ì¶œë ¥ ì§€ì› ëª¨ë¸",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 128000
            },
            "chatgpt-4o": {
                "name": "chatgpt-4o",
                "description": "ChatGPTì—ì„œ ì‚¬ìš©ë˜ëŠ” GPT-4o ëª¨ë¸",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000
            }
        },
        "ğŸ’¡ Cost-Optimized Models": {
            "gpt-4.1-mini": {
                "name": "gpt-4.1-mini",
                "description": "ì§€ëŠ¥ì„±, ì†ë„, ë¹„ìš©ì˜ ê· í˜•ì„ ë§ì¶˜ ëª¨ë¸",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576,
                "size": "Medium"
            },
            "gpt-4.1-nano": {
                "name": "gpt-4.1-nano",
                "description": "ê°€ì¥ ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ GPT-4.1 ëª¨ë¸",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576,
                "size": "Small"
            },
            "gpt-4o-mini": {
                "name": "gpt-4o-mini",
                "description": "ì§‘ì¤‘ëœ ì‘ì—…ì„ ìœ„í•œ ë¹ ë¥´ê³  ì €ë ´í•œ ì†Œí˜• ëª¨ë¸",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000,
                "size": "Small"
            },
            "gpt-4o-mini-audio": {
                "name": "gpt-4o-mini-audio",
                "description": "ì˜¤ë””ì˜¤ ì…ì¶œë ¥ì´ ê°€ëŠ¥í•œ ì†Œí˜• ëª¨ë¸",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 128000,
                "size": "Small"
            },
            "gpt-3.5-turbo": {
                "name": "gpt-3.5-turbo",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë²”ìš© ëª¨ë¸ (ì¼ë°˜ì ì¸ ëŒ€í™”ì— ìµœì )",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 16385,
                "size": "Small"
            }
        }
    }

class AIHandler:
    """AI ì‘ë‹µ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str):
        if api_key:
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def get_ai_response(self, messages: List[Dict], model_name: str = "gpt-3.5-turbo", 
                       temperature: float = 0.7, uploaded_files: Optional[List] = None):
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì‘ë‹µ ìƒì„±"""
        try:
            model_templates = get_model_templates()
            
            # ì„ íƒëœ ëª¨ë¸ì˜ ì •ë³´ ì°¾ê¸°
            selected_model_info = None
            for category in model_templates.values():
                if model_name in category:
                    selected_model_info = category[model_name]
                    break
            
            if not selected_model_info:
                selected_model_info = {"max_tokens": 1000, "supports_streaming": True}
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
            system_message = {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            }
            
            # ë©”ì‹œì§€ êµ¬ì„±
            api_messages = [system_message] + messages
            
            # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            context_added = False
            if uploaded_files:
                file_context = "\n\nì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´:\n"
                for file_info in uploaded_files:
                    file_context += f"- {file_info}\n"
                
                if api_messages:
                    api_messages[-1]["content"] += file_context
                    context_added = True
            
            # í˜„ì¬ í¸ì§‘ ì¤‘ì¸ DataFrameì´ ìˆëŠ” ê²½ìš° ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            if st.session_state.current_df is not None:
                df_context = "\n\ní˜„ì¬ í¸ì§‘ ì¤‘ì¸ ë°ì´í„°í”„ë ˆì„ ì •ë³´:\n"
                df_context += f"- í–‰ ìˆ˜: {len(st.session_state.current_df)}\n"
                df_context += f"- ì—´ ìˆ˜: {len(st.session_state.current_df.columns)}\n"
                df_context += f"- ì»¬ëŸ¼ëª…: {', '.join(st.session_state.current_df.columns.tolist())}\n"
                df_context += f"- ìµœê·¼ í¸ì§‘ëœ ë°ì´í„° (ìµœëŒ€ 10í–‰):\n{st.session_state.current_df.head(10).to_string()}\n"
                
                if api_messages:
                    if not context_added:
                        api_messages[-1]["content"] += df_context
                    else:
                        api_messages[-1]["content"] += df_context
            
            # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° ì„¤ì •
            api_params = {
                "model": model_name,
                "messages": api_messages,
                "max_tokens": selected_model_info["max_tokens"],
            }
            
            # Reasoning ëª¨ë¸ì˜ ê²½ìš° temperature ì§€ì›í•˜ì§€ ì•ŠìŒ
            if not model_name.startswith("o1"):
                api_params["temperature"] = temperature
            
            # ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ì„¤ì •
            if selected_model_info["supports_streaming"]:
                api_params["stream"] = True
            
            response = self.client.chat.completions.create(**api_params)
            
            return response
        except Exception as e:
            return f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" 