from dotenv import load_dotenv
from openai import OpenAI
import os


class aiModels:
    def __init__(self):
        load_dotenv()
        API_KEY = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=API_KEY)


    def get_model_templates(self):
        """OpenAI ëª¨ë¸ í…œí”Œë¦¿ ì •ì˜ - 2025ë…„ ìµœì‹  ëª¨ë¸ ë°˜ì˜"""
        return {
        "ğŸ§  Reasoning Models": {
            "o3": {
                "name": "o3",
                "description": "ê°€ì¥ ê°•ë ¥í•œ ì¶”ë¡  ëª¨ë¸ (ë³µì¡í•œ ìˆ˜í•™, ê³¼í•™, ì½”ë”© ë¬¸ì œì— ìµœì í™”)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o4-mini": {
                "name": "o4-mini",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì¶”ë¡  ëª¨ë¸ (ë©€í‹°ëª¨ë‹¬ ì§€ì›, ë„êµ¬ í†µí•©)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o3-mini": {
                "name": "o3-mini",
                "description": "o3ì˜ ì†Œí˜• ëŒ€ì•ˆ ëª¨ë¸ (ì¶”ë¡  ëŠ¥ë ¥ ìœ ì§€, ë¹„ìš© íš¨ìœ¨ì )",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o1": {
                "name": "o1",
                "description": "ì´ì „ o-ì‹œë¦¬ì¦ˆ ì¶”ë¡  ëª¨ë¸ (ì•ˆì •ì ì¸ ì¶”ë¡  ì„±ëŠ¥)",
                "max_tokens": 100000,
                "supports_streaming": False,
                "context_window": 200000
            },
            "o1-mini": {
                "name": "o1-mini",
                "description": "o1ì˜ ì†Œí˜• ëŒ€ì•ˆ (ì½”ë”© ë° ìˆ˜í•™ ë¬¸ì œì— íŠ¹í™”) - Deprecated",
                "max_tokens": 65536,
                "supports_streaming": False,
                "context_window": 128000,
                "deprecated": True
            }
        },
        "ğŸš€ Flagship Chat Models": {
            "gpt-4.1": {
                "name": "gpt-4.1",
                "description": "ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ í”Œë˜ê·¸ì‹­ GPT ëª¨ë¸ (ìµœê³  ì„±ëŠ¥)",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576
            },
            "gpt-4o": {
                "name": "gpt-4o",
                "description": "ë¹ ë¥´ê³  ì§€ëŠ¥ì ì´ë©° ìœ ì—°í•œ GPT ëª¨ë¸ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000
            },
            "gpt-4o-audio": {
                "name": "gpt-4o-audio",
                "description": "GPT-4o ì˜¤ë””ì˜¤ ì…ì¶œë ¥ ì§€ì› ëª¨ë¸",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 128000
            },
            "chatgpt-4o": {
                "name": "chatgpt-4o",
                "description": "ChatGPTì—ì„œ ì‚¬ìš©ë˜ëŠ” GPT-4o ëª¨ë¸",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000
            }
        },
        "ğŸ’¡ Cost-Optimized Models": {
            "gpt-4.1-mini": {
                "name": "gpt-4.1-mini",
                "description": "ì§€ëŠ¥ì„±, ì†ë„, ë¹„ìš©ì˜ ê· í˜•ì„ ë§ì¶˜ ëª¨ë¸",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576,
                "size": "Medium"
            },
            "gpt-4.1-nano": {
                "name": "gpt-4.1-nano",
                "description": "ê°€ì¥ ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ GPT-4.1 ëª¨ë¸",
                "max_tokens": 32768,
                "supports_streaming": True,
                "context_window": 1047576,
                "size": "Small"
            },
            "gpt-4o-mini": {
                "name": "gpt-4o-mini",
                "description": "ì§‘ì¤‘ëœ ì‘ì—…ì„ ìœ„í•œ ë¹ ë¥´ê³  ì €ë ´í•œ ì†Œí˜• ëª¨ë¸",
                "max_tokens": 16384,
                "supports_streaming": True,
                "context_window": 128000,
                "size": "Small"
            },
            "gpt-4o-mini-audio": {
                "name": "gpt-4o-mini-audio",
                "description": "ì˜¤ë””ì˜¤ ì…ì¶œë ¥ì´ ê°€ëŠ¥í•œ ì†Œí˜• ëª¨ë¸",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 128000,
                "size": "Small"
            },
            "gpt-3.5-turbo": {
                "name": "gpt-3.5-turbo",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë²”ìš© ëª¨ë¸ (ì¼ë°˜ì ì¸ ëŒ€í™”ì— ìµœì )",
                "max_tokens": 4096,
                "supports_streaming": True,
                "context_window": 16385,
                "size": "Small"
            }
        }
    }
    
    def get_response(self, prompt):
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

